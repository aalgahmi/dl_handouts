{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "Transfer learning is a technique where a model developed for a specific task is reused as the starting point for another model on a second (but somehow related) task. It involves taking a pre-trained model, which has already learned features from a large dataset, and fine-tuning it for a different but related task. This is particularly useful when the second task has limited labeled data, as the pre-trained model can leverage its learned features to generalize well on the new task with less data.\n",
    "\n",
    "There are typically two main approaches to transfer learning:\n",
    "\n",
    "* **Feature Extraction:** In this approach, the pre-trained model is used as a fixed feature extractor. The weights of the pre-trained layers are frozen, and only the final layers are modified and trained on the new task. In other words, the pre-trained model can be thought of as having both a base and a top. The base is frozen to prevent the backpropagation algorithm from changing its trained parameters, and the top is replaced with a new one, and is the only part of the model trained on the new data.\n",
    "\n",
    "* **Fine-tuning:** In this approach, the pre-trained model is further trained on the new task, and the weights of some or all layers are updated during training. This allows the model to adapt to the specific characteristics of the new dataset.\n",
    "\n",
    "Transfer learning is widely used in computer vision and natural language processing, where large pre-trained models, such as ConvNets for images or pre-trained language models for text, are fine-tuned for specific applications.\n",
    "\n",
    "This strategy is often employed when there isn't enough data, time, and/or resources to train a full-scale model from scratch. The `torchvision.models` library comes with many pre-trained computer vision models. You can find a list of these pre-trained models [here](https://pytorch.org/vision/0.9/models.html).\n",
    "\n",
    "This notebook applies transfer learning to the two examples of the previous handout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dogs vs cats\n",
    "\n",
    "One of the pretrained models implemented by `torchvision.models` is the VGG16 ConvNet. VGG16 is a deep convolutional neural network architecture introduced by the Visual Geometry Group at the University of Oxford and presented in a paper from 2015 titled 'Very Deep Convolutional Networks for Large-Scale Image Recognition' by Karen Simonyan and Andrew Zisserman. It is known for its simplicity and uniform architecture, and consists of 16 weight layers, including 13 convolutional layers and 3 fully connected layers.\n",
    "\n",
    "Let's use VGG16 with the dogs vs. cats example, starting by reusing the data module from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import lightning as L\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import opendatasets as od\n",
    "import os, shutil, pathlib\n",
    "\n",
    "torch.random.manual_seed(17);\n",
    "\n",
    "class DogsVsCatsDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_path='./datasets', transform = transforms.Compose([\n",
    "            transforms.Resize(size=(128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ]), train_transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.train_transform = transform if train_transform is None else train_transform\n",
    "\n",
    "    def make_subset(self, subset_name, start_index, end_index):\n",
    "        for category in (\"cat\", \"dog\"):\n",
    "            dir = self.dest_dir / subset_name / category\n",
    "            os.makedirs(dir)\n",
    "            fnames = [ f\"{category}.{i}.jpg\" for i in range(start_index, end_index) ]\n",
    "            for fname in fnames:\n",
    "                shutil.copyfile(src=self.src_dir / fname, dst=dir / fname)\n",
    "                        \n",
    "    def prepare_data(self):\n",
    "        dataset_url = 'https://www.kaggle.com/c/dogs-vs-cats/data'\n",
    "        od.download(dataset_url, data_dir=self.data_path)\n",
    "        \n",
    "        self.src_dir = pathlib.Path(self.data_path + \"/dogs-vs-cats/train\")\n",
    "        self.dest_dir = pathlib.Path(self.data_path + \"/dogs-vs-cats/processed\")\n",
    "        \n",
    "        if not os.path.exists(self.dest_dir):\n",
    "            self.make_subset(\"train\", start_index=0, end_index=8000)\n",
    "            self.make_subset(\"validation\", start_index=8000, end_index=9500)\n",
    "            self.make_subset(\"test\", start_index=9500, end_index=11000)\n",
    "        else:\n",
    "            print(\"Skipping!\", self.dest_dir, \"already exists.\")\n",
    "            \n",
    "    def setup(self, stage=None):\n",
    "        self.target_transform = transforms.Lambda(lambda y: torch.tensor([y]).float())\n",
    "        \n",
    "        self.ds_train = datasets.ImageFolder(f\"{self.dest_dir}/train\", transform=self.train_transform) \n",
    "        self.ds_val = datasets.ImageFolder(f\"{self.dest_dir}/validation\", transform=self.transform)\n",
    "        self.ds_test = datasets.ImageFolder(f\"{self.dest_dir}/test\", transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=128, num_workers=4, shuffle=True, \n",
    "                          persistent_workers=True, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=128, num_workers=4, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=128, num_workers=4, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we download and instantiate a pre-trained vgg16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "VGG                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       1,792\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Conv2d: 2-3                       36,928\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─MaxPool2d: 2-5                    --\n",
       "│    └─Conv2d: 2-6                       73,856\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─Conv2d: 2-8                       147,584\n",
       "│    └─ReLU: 2-9                         --\n",
       "│    └─MaxPool2d: 2-10                   --\n",
       "│    └─Conv2d: 2-11                      295,168\n",
       "│    └─ReLU: 2-12                        --\n",
       "│    └─Conv2d: 2-13                      590,080\n",
       "│    └─ReLU: 2-14                        --\n",
       "│    └─Conv2d: 2-15                      590,080\n",
       "│    └─ReLU: 2-16                        --\n",
       "│    └─MaxPool2d: 2-17                   --\n",
       "│    └─Conv2d: 2-18                      1,180,160\n",
       "│    └─ReLU: 2-19                        --\n",
       "│    └─Conv2d: 2-20                      2,359,808\n",
       "│    └─ReLU: 2-21                        --\n",
       "│    └─Conv2d: 2-22                      2,359,808\n",
       "│    └─ReLU: 2-23                        --\n",
       "│    └─MaxPool2d: 2-24                   --\n",
       "│    └─Conv2d: 2-25                      2,359,808\n",
       "│    └─ReLU: 2-26                        --\n",
       "│    └─Conv2d: 2-27                      2,359,808\n",
       "│    └─ReLU: 2-28                        --\n",
       "│    └─Conv2d: 2-29                      2,359,808\n",
       "│    └─ReLU: 2-30                        --\n",
       "│    └─MaxPool2d: 2-31                   --\n",
       "├─AdaptiveAvgPool2d: 1-2                 --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Linear: 2-32                      102,764,544\n",
       "│    └─ReLU: 2-33                        --\n",
       "│    └─Dropout: 2-34                     --\n",
       "│    └─Linear: 2-35                      16,781,312\n",
       "│    └─ReLU: 2-36                        --\n",
       "│    └─Dropout: 2-37                     --\n",
       "│    └─Linear: 2-38                      4,097,000\n",
       "=================================================================\n",
       "Total params: 138,357,544\n",
       "Trainable params: 138,357,544\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two main big blocks separated by an `AdaptiveAvgPool2d` layer. You can think of the first sequential block as the base (or feature extractor) and of the second sequential block as the top (or classifier). Transfer learning involves changing/replacing the top block to fit the new task at hand. To do that, we freeze the network first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sure that none of the pre-trained weights and biases will be affected when the model is trained again on the new data. Next, we change or replace the classifier block, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace the last layer of the classifier with one that has two units instead of the original 1000, we do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=2, bias=True)\n",
    "vgg16.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how to replace the while classifier block with a new untrained one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=256, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=256, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to do that, let's put it all inside a Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16BasedClassifier(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        \n",
    "        self.pretrained_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        self.pretrained_model.eval()\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pretrained_model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=256, out_features=2, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pretrained_model(x)\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, accuracy, loss_lbl, accuracy_lbl):\n",
    "        X, y = batch\n",
    "        logits = self(X)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        y_hat = torch.argmax(logits, dim=1)\n",
    "        self.log(loss_lbl, loss, prog_bar=True)\n",
    "        self.log(accuracy_lbl, accuracy(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, self.train_accuracy, \"loss\", \"accuracy\")\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        self.log(\"accuracy\", self.train_accuracy.compute())\n",
    "        self.train_accuracy.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, self.val_accuracy, \"val_loss\", \"val_accuracy\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"val_accuracy\", self.val_accuracy.compute())\n",
    "        self.val_accuracy.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, self.test_accuracy, \"test_loss\", \"test_accuracy\")\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test_accuracy\", self.test_accuracy.compute())\n",
    "        self.test_accuracy.reset()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop(self.parameters(), lr=1e-4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this class, here is a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_based_model = VGG16BasedClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "VGG16BasedClassifier                     --\n",
       "├─MulticlassAccuracy: 1-1                --\n",
       "├─MulticlassAccuracy: 1-2                --\n",
       "├─MulticlassAccuracy: 1-3                --\n",
       "├─VGG: 1-4                               --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  (1,792)\n",
       "│    │    └─ReLU: 3-2                    --\n",
       "│    │    └─Conv2d: 3-3                  (36,928)\n",
       "│    │    └─ReLU: 3-4                    --\n",
       "│    │    └─MaxPool2d: 3-5               --\n",
       "│    │    └─Conv2d: 3-6                  (73,856)\n",
       "│    │    └─ReLU: 3-7                    --\n",
       "│    │    └─Conv2d: 3-8                  (147,584)\n",
       "│    │    └─ReLU: 3-9                    --\n",
       "│    │    └─MaxPool2d: 3-10              --\n",
       "│    │    └─Conv2d: 3-11                 (295,168)\n",
       "│    │    └─ReLU: 3-12                   --\n",
       "│    │    └─Conv2d: 3-13                 (590,080)\n",
       "│    │    └─ReLU: 3-14                   --\n",
       "│    │    └─Conv2d: 3-15                 (590,080)\n",
       "│    │    └─ReLU: 3-16                   --\n",
       "│    │    └─MaxPool2d: 3-17              --\n",
       "│    │    └─Conv2d: 3-18                 (1,180,160)\n",
       "│    │    └─ReLU: 3-19                   --\n",
       "│    │    └─Conv2d: 3-20                 (2,359,808)\n",
       "│    │    └─ReLU: 3-21                   --\n",
       "│    │    └─Conv2d: 3-22                 (2,359,808)\n",
       "│    │    └─ReLU: 3-23                   --\n",
       "│    │    └─MaxPool2d: 3-24              --\n",
       "│    │    └─Conv2d: 3-25                 (2,359,808)\n",
       "│    │    └─ReLU: 3-26                   --\n",
       "│    │    └─Conv2d: 3-27                 (2,359,808)\n",
       "│    │    └─ReLU: 3-28                   --\n",
       "│    │    └─Conv2d: 3-29                 (2,359,808)\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    │    └─MaxPool2d: 3-31              --\n",
       "│    └─AdaptiveAvgPool2d: 2-2            --\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─Linear: 3-32                 6,422,784\n",
       "│    │    └─ReLU: 3-33                   --\n",
       "│    │    └─Dropout: 3-34                --\n",
       "│    │    └─Linear: 3-35                 514\n",
       "=================================================================\n",
       "Total params: 21,137,986\n",
       "Trainable params: 6,423,298\n",
       "Non-trainable params: 14,714,688\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vgg16_based_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train this model. When we do so, only the replaced classifier block will be affected. The rest is frozen. We'll use the early stopping callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538,
     "referenced_widgets": [
      "c4c7f297e8d04292bf68ba19ee2e779a",
      "f1a1175c4c3b42d4a84cd97d6932cc64",
      "cbf3cf3223454827940820925d5231f7",
      "ef4f269918914a30af721ce573445538",
      "8a405c15fb83497aa9651c60a877a83c",
      "94c238923ff54b66a89253266cb5e647",
      "a088d3cdcba14259800cc97065fb5791",
      "a217d89c1c2d4afe86c899245d3fb5cf",
      "557c90f106234ee282cce33c5938e76b",
      "5b2a96faa0bc40d7a7abed02601b9a80",
      "24ca5023f1424d7391d43550d3f9a22a"
     ]
    },
    "id": "4GrPm_-LmU3t",
    "outputId": "781789ba-b9b4-4590-eac9-91e622ef385c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
      "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
      "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
      "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
      "3 | pretrained_model | VGG                | 21.1 M\n",
      "--------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "21.1 M    Total params\n",
      "84.552    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                   | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95823f151ee24258a2845cab3936a795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                          | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "dogs_vs_cats_dm = DogsVsCatsDataModule()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5, callbacks=[\n",
    "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "])\n",
    "trainer.fit(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538,
     "referenced_widgets": [
      "c4c7f297e8d04292bf68ba19ee2e779a",
      "f1a1175c4c3b42d4a84cd97d6932cc64",
      "cbf3cf3223454827940820925d5231f7",
      "ef4f269918914a30af721ce573445538",
      "8a405c15fb83497aa9651c60a877a83c",
      "94c238923ff54b66a89253266cb5e647",
      "a088d3cdcba14259800cc97065fb5791",
      "a217d89c1c2d4afe86c899245d3fb5cf",
      "557c90f106234ee282cce33c5938e76b",
      "5b2a96faa0bc40d7a7abed02601b9a80",
      "24ca5023f1424d7391d43550d3f9a22a"
     ]
    },
    "id": "4GrPm_-LmU3t",
    "outputId": "781789ba-b9b4-4590-eac9-91e622ef385c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
      "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
      "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
      "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
      "3 | pretrained_model | VGG                | 21.1 M\n",
      "--------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "21.1 M    Total params\n",
      "84.552    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                   | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95823f151ee24258a2845cab3936a795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                          | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "dogs_vs_cats_dm = DogsVsCatsDataModule()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5, callbacks=[\n",
    "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "])\n",
    "trainer.fit(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
      "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efe7dc971e54955bd0e66e5fb32a7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                           | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9393333196640015     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14621175825595856    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9393333196640015    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14621175825595856   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.14621175825595856, 'test_accuracy': 0.9393333196640015}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model significantly outperforms our previous from-scratch models by capitalizing on a pre-trained VGG16 model. The VGG16 model has been previously trained on the extensive ImageNet dataset, a widely used dataset for training and evaluating computer vision models, especially for image classification tasks. Given the richness of cat and dog images in the ImageNet dataset, this pretrained model proves highly effective in our specific task.\n",
    "\n",
    "\n",
    "\n",
    "### Fine-tuning a pretrained model\n",
    "As an optional, but widely used, step, we can improve our this pre-trained model by fine-tuning it. This is done by unfreezing all or part of the pre-trained base block and re-training it on the new data with a very slow learning rate. The whole process translates to the following steps:\n",
    "* Instantiate the base model\n",
    "* Freeze it\n",
    "* Add a new top to it\n",
    "* Train the part we added\n",
    "* Unfreeze some layers on the base model. Don't unfreeze any batch normalization layer.\n",
    "* Jointly train both the new unfrozen part of the base and the top part of the model using a very slow learning rate.\n",
    "\n",
    "We already did the top four steps. Let's do the last two. First we unfreeze the top two convolutional layers of the base block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [26, 28]:\n",
    "    for param in vgg16_based_model.pretrained_model.features[i].parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having unfrozen these two layers, we train the model again with a slower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
      "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
      "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
      "3 | pretrained_model | VGG                | 21.1 M\n",
      "--------------------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "10.0 M    Non-trainable params\n",
      "21.1 M    Total params\n",
      "84.552    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
      "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                   | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3cf40bea9e418fa8bfd8a0cf2e7a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                          | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "vgg16_based_model.lr = 0.0001\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2, callbacks=[\n",
    "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "])\n",
    "trainer.fit(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate this fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
      "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8a7b011164403ba5376b638dda6c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                           | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9380000233650208     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1642094999551773     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9380000233650208    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1642094999551773    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.1642094999551773, 'test_accuracy': 0.9380000233650208}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer tissue detection\n",
    "As a second example, let's use another popular pre-trained network for detecting cancer tissues using the PCam dataset. ResNet50 is a variant of the ResNet (Residual Network) architecture, a deep neural network architecture that introduced residual blocks, which help address the challenges of training very deep neural networks. ResNet50 consists of 50 layers, making it a relatively deep neural network. Due to its success and efficiency, it serves as a benchmark model in the field of deep learning.\n",
    "\n",
    "Let's download and summarize a pre-trained ResNet50 model from `torchvision.models`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ResNet                                   --\n",
       "├─Conv2d: 1-1                            9,408\n",
       "├─BatchNorm2d: 1-2                       128\n",
       "├─ReLU: 1-3                              --\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─Bottleneck: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  4,096\n",
       "│    │    └─BatchNorm2d: 3-2             128\n",
       "│    │    └─Conv2d: 3-3                  36,864\n",
       "│    │    └─BatchNorm2d: 3-4             128\n",
       "│    │    └─Conv2d: 3-5                  16,384\n",
       "│    │    └─BatchNorm2d: 3-6             512\n",
       "│    │    └─ReLU: 3-7                    --\n",
       "│    │    └─Sequential: 3-8              16,896\n",
       "│    └─Bottleneck: 2-2                   --\n",
       "│    │    └─Conv2d: 3-9                  16,384\n",
       "│    │    └─BatchNorm2d: 3-10            128\n",
       "│    │    └─Conv2d: 3-11                 36,864\n",
       "│    │    └─BatchNorm2d: 3-12            128\n",
       "│    │    └─Conv2d: 3-13                 16,384\n",
       "│    │    └─BatchNorm2d: 3-14            512\n",
       "│    │    └─ReLU: 3-15                   --\n",
       "│    └─Bottleneck: 2-3                   --\n",
       "│    │    └─Conv2d: 3-16                 16,384\n",
       "│    │    └─BatchNorm2d: 3-17            128\n",
       "│    │    └─Conv2d: 3-18                 36,864\n",
       "│    │    └─BatchNorm2d: 3-19            128\n",
       "│    │    └─Conv2d: 3-20                 16,384\n",
       "│    │    └─BatchNorm2d: 3-21            512\n",
       "│    │    └─ReLU: 3-22                   --\n",
       "├─Sequential: 1-6                        --\n",
       "│    └─Bottleneck: 2-4                   --\n",
       "│    │    └─Conv2d: 3-23                 32,768\n",
       "│    │    └─BatchNorm2d: 3-24            256\n",
       "│    │    └─Conv2d: 3-25                 147,456\n",
       "│    │    └─BatchNorm2d: 3-26            256\n",
       "│    │    └─Conv2d: 3-27                 65,536\n",
       "│    │    └─BatchNorm2d: 3-28            1,024\n",
       "│    │    └─ReLU: 3-29                   --\n",
       "│    │    └─Sequential: 3-30             132,096\n",
       "│    └─Bottleneck: 2-5                   --\n",
       "│    │    └─Conv2d: 3-31                 65,536\n",
       "│    │    └─BatchNorm2d: 3-32            256\n",
       "│    │    └─Conv2d: 3-33                 147,456\n",
       "│    │    └─BatchNorm2d: 3-34            256\n",
       "│    │    └─Conv2d: 3-35                 65,536\n",
       "│    │    └─BatchNorm2d: 3-36            1,024\n",
       "│    │    └─ReLU: 3-37                   --\n",
       "│    └─Bottleneck: 2-6                   --\n",
       "│    │    └─Conv2d: 3-38                 65,536\n",
       "│    │    └─BatchNorm2d: 3-39            256\n",
       "│    │    └─Conv2d: 3-40                 147,456\n",
       "│    │    └─BatchNorm2d: 3-41            256\n",
       "│    │    └─Conv2d: 3-42                 65,536\n",
       "│    │    └─BatchNorm2d: 3-43            1,024\n",
       "│    │    └─ReLU: 3-44                   --\n",
       "│    └─Bottleneck: 2-7                   --\n",
       "│    │    └─Conv2d: 3-45                 65,536\n",
       "│    │    └─BatchNorm2d: 3-46            256\n",
       "│    │    └─Conv2d: 3-47                 147,456\n",
       "│    │    └─BatchNorm2d: 3-48            256\n",
       "│    │    └─Conv2d: 3-49                 65,536\n",
       "│    │    └─BatchNorm2d: 3-50            1,024\n",
       "│    │    └─ReLU: 3-51                   --\n",
       "├─Sequential: 1-7                        --\n",
       "│    └─Bottleneck: 2-8                   --\n",
       "│    │    └─Conv2d: 3-52                 131,072\n",
       "│    │    └─BatchNorm2d: 3-53            512\n",
       "│    │    └─Conv2d: 3-54                 589,824\n",
       "│    │    └─BatchNorm2d: 3-55            512\n",
       "│    │    └─Conv2d: 3-56                 262,144\n",
       "│    │    └─BatchNorm2d: 3-57            2,048\n",
       "│    │    └─ReLU: 3-58                   --\n",
       "│    │    └─Sequential: 3-59             526,336\n",
       "│    └─Bottleneck: 2-9                   --\n",
       "│    │    └─Conv2d: 3-60                 262,144\n",
       "│    │    └─BatchNorm2d: 3-61            512\n",
       "│    │    └─Conv2d: 3-62                 589,824\n",
       "│    │    └─BatchNorm2d: 3-63            512\n",
       "│    │    └─Conv2d: 3-64                 262,144\n",
       "│    │    └─BatchNorm2d: 3-65            2,048\n",
       "│    │    └─ReLU: 3-66                   --\n",
       "│    └─Bottleneck: 2-10                  --\n",
       "│    │    └─Conv2d: 3-67                 262,144\n",
       "│    │    └─BatchNorm2d: 3-68            512\n",
       "│    │    └─Conv2d: 3-69                 589,824\n",
       "│    │    └─BatchNorm2d: 3-70            512\n",
       "│    │    └─Conv2d: 3-71                 262,144\n",
       "│    │    └─BatchNorm2d: 3-72            2,048\n",
       "│    │    └─ReLU: 3-73                   --\n",
       "│    └─Bottleneck: 2-11                  --\n",
       "│    │    └─Conv2d: 3-74                 262,144\n",
       "│    │    └─BatchNorm2d: 3-75            512\n",
       "│    │    └─Conv2d: 3-76                 589,824\n",
       "│    │    └─BatchNorm2d: 3-77            512\n",
       "│    │    └─Conv2d: 3-78                 262,144\n",
       "│    │    └─BatchNorm2d: 3-79            2,048\n",
       "│    │    └─ReLU: 3-80                   --\n",
       "│    └─Bottleneck: 2-12                  --\n",
       "│    │    └─Conv2d: 3-81                 262,144\n",
       "│    │    └─BatchNorm2d: 3-82            512\n",
       "│    │    └─Conv2d: 3-83                 589,824\n",
       "│    │    └─BatchNorm2d: 3-84            512\n",
       "│    │    └─Conv2d: 3-85                 262,144\n",
       "│    │    └─BatchNorm2d: 3-86            2,048\n",
       "│    │    └─ReLU: 3-87                   --\n",
       "│    └─Bottleneck: 2-13                  --\n",
       "│    │    └─Conv2d: 3-88                 262,144\n",
       "│    │    └─BatchNorm2d: 3-89            512\n",
       "│    │    └─Conv2d: 3-90                 589,824\n",
       "│    │    └─BatchNorm2d: 3-91            512\n",
       "│    │    └─Conv2d: 3-92                 262,144\n",
       "│    │    └─BatchNorm2d: 3-93            2,048\n",
       "│    │    └─ReLU: 3-94                   --\n",
       "├─Sequential: 1-8                        --\n",
       "│    └─Bottleneck: 2-14                  --\n",
       "│    │    └─Conv2d: 3-95                 524,288\n",
       "│    │    └─BatchNorm2d: 3-96            1,024\n",
       "│    │    └─Conv2d: 3-97                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-98            1,024\n",
       "│    │    └─Conv2d: 3-99                 1,048,576\n",
       "│    │    └─BatchNorm2d: 3-100           4,096\n",
       "│    │    └─ReLU: 3-101                  --\n",
       "│    │    └─Sequential: 3-102            2,101,248\n",
       "│    └─Bottleneck: 2-15                  --\n",
       "│    │    └─Conv2d: 3-103                1,048,576\n",
       "│    │    └─BatchNorm2d: 3-104           1,024\n",
       "│    │    └─Conv2d: 3-105                2,359,296\n",
       "│    │    └─BatchNorm2d: 3-106           1,024\n",
       "│    │    └─Conv2d: 3-107                1,048,576\n",
       "│    │    └─BatchNorm2d: 3-108           4,096\n",
       "│    │    └─ReLU: 3-109                  --\n",
       "│    └─Bottleneck: 2-16                  --\n",
       "│    │    └─Conv2d: 3-110                1,048,576\n",
       "│    │    └─BatchNorm2d: 3-111           1,024\n",
       "│    │    └─Conv2d: 3-112                2,359,296\n",
       "│    │    └─BatchNorm2d: 3-113           1,024\n",
       "│    │    └─Conv2d: 3-114                1,048,576\n",
       "│    │    └─BatchNorm2d: 3-115           4,096\n",
       "│    │    └─ReLU: 3-116                  --\n",
       "├─AdaptiveAvgPool2d: 1-9                 --\n",
       "├─Linear: 1-10                           2,049,000\n",
       "=================================================================\n",
       "Total params: 25,557,032\n",
       "Trainable params: 25,557,032\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "summary(resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a deeper neural network. We can also print it to get the indexes and names of the layers and blocks that make it. This will be useful when we make changes to its top layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last layer, its name is `fc` and it consists of 1000 units because of the 1000 classes of the ImageNet dataset it was trained on. This is the layer we need to replace to make this network work for our PCam dataset. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and preparing the data\n",
    "\n",
    "We'll start by fetching and preparing the data, leveraging some of the code we had in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./datasets/histopathologic-cancer-detection\" (use force=True to force download)\n",
      "Skipping! datasets/histopathologic-cancer-detection/processed already exists.\n"
     ]
    }
   ],
   "source": [
    "def make_subset(subset, start_index, end_index, images, labels):\n",
    "    categories = {0: \"0_normal\", 1: \"1_abnormal\" }\n",
    "    for i in range(start_index, end_index):\n",
    "        category = categories[labels[i]]\n",
    "        dir = dest_dir / subset / category\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        fname = f\"{images[i]}.tif\"\n",
    "        shutil.copyfile(src=src_dir / fname, dst=dir / fname)\n",
    "        \n",
    "# Downloading the data from kaggle if needed\n",
    "dataset_url = 'https://www.kaggle.com/c/histopathologic-cancer-detection'\n",
    "od.download(dataset_url, data_dir='./datasets')\n",
    "\n",
    "# Getting the actual labels\n",
    "data_path = './datasets/histopathologic-cancer-detection'\n",
    "labels = pd.read_csv(data_path + '/train_labels.csv')\n",
    "labels = labels.set_index('id')\n",
    "\n",
    "# Selecting random 20,000 images and splitting them into three sets\n",
    "src_dir = pathlib.Path(data_path + \"/train\")\n",
    "dest_dir = pathlib.Path(data_path + \"/processed\")\n",
    "\n",
    "train_image_files = os.listdir(src_dir)\n",
    "selected_images = [\n",
    "    train_image_files[f].split('.')[0] \n",
    "    for f in torch.randperm(len(train_image_files))[:20000]\n",
    "]\n",
    "\n",
    "selected_labels = labels.loc[selected_images]['label'].values\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    make_subset(\"train\", 0, 16000, selected_images, selected_labels)\n",
    "    make_subset(\"validation\", 16000, 18000, selected_images, selected_labels)\n",
    "    make_subset(\"test\", 18000, 20000, selected_images, selected_labels)\n",
    "else:\n",
    "    print(\"Skipping!\", dest_dir, \"already exists.\")\n",
    "\n",
    "# Creating the datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds_train = datasets.ImageFolder(f\"{dest_dir}/train\", transform=train_transform) \n",
    "ds_val = datasets.ImageFolder(f\"{dest_dir}/validation\", transform=transform)\n",
    "ds_test = datasets.ImageFolder(f\"{dest_dir}/test\", transform=transform)\n",
    "\n",
    "# Creating the corresponding data loaders\n",
    "dl_train = DataLoader(ds_train, batch_size=256, num_workers=4, \n",
    "                      shuffle=True, persistent_workers=True, \n",
    "                      drop_last=True, pin_memory=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=256, num_workers=4, \n",
    "                    persistent_workers=True, pin_memory=True)\n",
    "dl_test = DataLoader(ds_test, batch_size=256, num_workers=4, \n",
    "                     persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a ResNet50 model\n",
    "\n",
    "Next, we construct a Lightning model based on ResNet50. Similar to our previous approach, once we download and instantiate the pretrained ResNet50, we freeze all its parameters. This prevents the pretrained weights from being updated during the training on the PCam dataset. Finally, we replace the last (top) linear layer, named `fc`, with a new one containing only 2 units instead of the original 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50BasedClassifier(L.LightningModule):\n",
    "\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        \n",
    "        self.pretrained_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.pretrained_model.eval()\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pretrained_model.fc = nn.Linear(2048, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pretrained_model(x)\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, accuracy, loss_lbl, accuracy_lbl):\n",
    "        X, y = batch\n",
    "        logits = self(X)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        y_hat = torch.argmax(logits, dim=1)\n",
    "        self.log(loss_lbl, loss, prog_bar=True)\n",
    "        self.log(accuracy_lbl, accuracy(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, self.train_accuracy, \"loss\", \"accuracy\")\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        self.log(\"accuracy\", self.train_accuracy.compute())\n",
    "        self.train_accuracy.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, self.val_accuracy, \"val_loss\", \"val_accuracy\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"val_accuracy\", self.val_accuracy.compute())\n",
    "        self.val_accuracy.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, self.test_accuracy, \"test_loss\", \"test_accuracy\")\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test_accuracy\", self.test_accuracy.compute())\n",
    "        self.test_accuracy.reset()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train this model on the PCam dataset. It's important to note that we should not expect the same level of performance as in the previous example because the PCam data differs significantly from the ImageNet dataset on which the base ResNet50 model is trained. However with enough epochs, we still anticipate good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538,
     "referenced_widgets": [
      "c4c7f297e8d04292bf68ba19ee2e779a",
      "f1a1175c4c3b42d4a84cd97d6932cc64",
      "cbf3cf3223454827940820925d5231f7",
      "ef4f269918914a30af721ce573445538",
      "8a405c15fb83497aa9651c60a877a83c",
      "94c238923ff54b66a89253266cb5e647",
      "a088d3cdcba14259800cc97065fb5791",
      "a217d89c1c2d4afe86c899245d3fb5cf",
      "557c90f106234ee282cce33c5938e76b",
      "5b2a96faa0bc40d7a7abed02601b9a80",
      "24ca5023f1424d7391d43550d3f9a22a"
     ]
    },
    "id": "4GrPm_-LmU3t",
    "outputId": "781789ba-b9b4-4590-eac9-91e622ef385c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
      "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
      "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
      "3 | pretrained_model | ResNet             | 23.5 M\n",
      "--------------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.049    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                   | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41231da45abb45dbb4fe2541733d4962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                          | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                        | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet50_based_model = ResNet50BasedClassifier()\n",
    "\n",
    "trainer = L.Trainer(max_epochs=15, callbacks=[\n",
    "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "])\n",
    "\n",
    "trainer.fit(resnet50_based_model, train_dataloaders=dl_train, val_dataloaders=dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034f319f24a14edbb3a95ce92aa906d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                           | 0/? [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7854999899864197     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.47510087490081787    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7854999899864197    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.47510087490081787   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.47510087490081787, 'test_accuracy': 0.7854999899864197}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(resnet50_based_model, dataloaders=dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are comparable, if not slightly better, than the ones obtained from models trained from scratch. This suggests that even with a substantially different dataset, such as PCam, transfer learning remains a powerful and effective technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c4ab92bbc10d18b480eb40b47e751a2f458ae1b2b88576453c716fde62785ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
