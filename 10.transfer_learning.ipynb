{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrNxrqST5LV"
      },
      "source": [
        "Run locally or <a target=\"_blank\" href=\"https://colab.research.google.com/github/aalgahmi/dl_handouts/blob/main/10.transfer_learning.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo torchviz lightning opendatasets\n",
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "xYiA9hpxUGfV",
        "outputId": "c715b6b9-a7eb-4a64-ce87-97311cffca1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIkBsaq3T5Lc"
      },
      "source": [
        "# Transfer learning\n",
        "\n",
        "Transfer learning is a technique where a model developed for a specific task is reused as the starting point for another model on a second (but somehow related) task. It involves taking a pre-trained model, which has already learned features from a large dataset, and fine-tuning it for a different but related task. This is particularly useful when the second task has limited labeled data, as the pre-trained model can leverage its learned features to generalize well on the new task with less data.\n",
        "\n",
        "There are typically two main approaches to transfer learning:\n",
        "\n",
        "* **Feature Extraction:** In this approach, the pre-trained model is used as a fixed feature extractor. The weights of the pre-trained layers are frozen, and only the final layers are modified and trained on the new task. In other words, the pre-trained model can be thought of as having both a base and a top. The base is frozen to prevent the backpropagation algorithm from changing its trained parameters, and the top is replaced with a new one, and is the only part of the model trained on the new data.\n",
        "\n",
        "* **Fine-tuning:** In this approach, the pre-trained model is further trained on the new task, and the weights of some or all layers are updated during training. This allows the model to adapt to the specific characteristics of the new dataset.\n",
        "\n",
        "Transfer learning is widely used in computer vision and natural language processing, where large pre-trained models, such as ConvNets for images or pre-trained language models for text, are fine-tuned for specific applications.\n",
        "\n",
        "This strategy is often employed when there isn't enough data, time, and/or resources to train a full-scale model from scratch. The `torchvision.models` library comes with many pre-trained computer vision models. You can find a list of these pre-trained models [here](https://pytorch.org/vision/0.9/models.html).\n",
        "\n",
        "This notebook applies transfer learning to the two examples of the previous handout.\n",
        "\n",
        "But first we get the data. You'll need your Kaggle username and key for this. You can ignore this step if you already have it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "\n",
        "data_path = './datasets'\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/c/dogs-vs-cats/data'\n",
        "od.download(dataset_url, data_dir=data_path)\n"
      ],
      "metadata": {
        "id": "SggldpL3ffEe",
        "outputId": "94c5fd98-f83f-413f-86f2-0f57d8f2c644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: aalgahmi4dl\n",
            "Your Kaggle Key: ··········\n",
            "Downloading dogs-vs-cats.zip to ./datasets/dogs-vs-cats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 812M/812M [00:07<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting archive ./datasets/dogs-vs-cats/dogs-vs-cats.zip to ./datasets/dogs-vs-cats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./datasets/dogs-vs-cats/ && unzip -qq train.zip && cd -"
      ],
      "metadata": {
        "id": "yzfe1rfpfj-Z",
        "outputId": "feb5b93f-f4e4-4d7f-f2fe-29b9b6ad4606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "src_dir = pathlib.Path(\"./datasets/dogs-vs-cats/train\")\n",
        "dest_dir = pathlib.Path(\"./datasets/dogs-vs-cats/processed\")\n",
        "\n",
        "if not os.path.exists(dest_dir):\n",
        "    def make_subset(subset_name, start_index, end_index):\n",
        "        for category in (\"cat\", \"dog\"):\n",
        "            dir = dest_dir / subset_name / category\n",
        "            os.makedirs(dir)\n",
        "            fnames = [ f\"{category}.{i}.jpg\" for i in range(start_index, end_index) ]\n",
        "            for fname in fnames:\n",
        "                shutil.copyfile(src=src_dir / fname,\n",
        "                                dst=dir / fname)\n",
        "\n",
        "    make_subset(\"train\", start_index=0, end_index=8000)\n",
        "    make_subset(\"validation\", start_index=8000, end_index=9500)\n",
        "    make_subset(\"test\", start_index=9500, end_index=11000)\n",
        "else:\n",
        "    print(\"Skipping!\", dest_dir, \"already exists.\")"
      ],
      "metadata": {
        "id": "20xX_nSVdznt",
        "outputId": "cbd99750-5610-4784-a6e1-632100ff4970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "dataset_url = 'https://www.kaggle.com/c/histopathologic-cancer-detection'\n",
        "od.download(dataset_url, data_dir='./datasets')\n",
        "\n",
        "def make_subset(subset, start_index, end_index, images, labels):\n",
        "    categories = {0: \"0_normal\", 1: \"1_abnormal\" }\n",
        "    for i in range(start_index, end_index):\n",
        "        category = categories[labels[i]]\n",
        "        dir = dest_dir / subset / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        fname = f\"{images[i]}.tif\"\n",
        "        shutil.copyfile(src=src_dir / fname, dst=dir / fname)\n",
        "\n",
        "data_path = './datasets/histopathologic-cancer-detection'\n",
        "labels = pd.read_csv(data_path + '/train_labels.csv')\n",
        "labels = labels.set_index('id')\n",
        "\n",
        "src_dir = pathlib.Path(data_path + \"/train\")\n",
        "dest_dir = pathlib.Path(data_path + \"/processed\")\n",
        "\n",
        "train_image_files = os.listdir(src_dir)\n",
        "selected_images = [\n",
        "    train_image_files[f].split('.')[0]\n",
        "    for f in torch.randperm(len(train_image_files))[:20000]\n",
        "]\n",
        "\n",
        "selected_labels = labels.loc[selected_images]['label'].values\n",
        "\n",
        "len(selected_images), len(selected_labels)\n",
        "\n",
        "if not os.path.exists(dest_dir):\n",
        "    make_subset(\"train\", 0, 16000, selected_images, selected_labels)\n",
        "    make_subset(\"validation\", 16000, 18000, selected_images, selected_labels)\n",
        "    make_subset(\"test\", 18000, 20000, selected_images, selected_labels)\n",
        "else:\n",
        "    print(\"Skipping!\", dest_dir, \"already exists.\")"
      ],
      "metadata": {
        "id": "-7Rj0fX_eAog",
        "outputId": "9d540b74-8eaa-412d-d2ea-fa6eb58c4636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./datasets/histopathologic-cancer-detection\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSaBySdnT5Lc"
      },
      "source": [
        "## Dogs vs cats\n",
        "\n",
        "One of the pretrained models implemented by `torchvision.models` is the VGG16 ConvNet. VGG16 is a deep convolutional neural network architecture introduced by the Visual Geometry Group at the University of Oxford and presented in a paper from 2015 titled 'Very Deep Convolutional Networks for Large-Scale Image Recognition' by Karen Simonyan and Andrew Zisserman. It is known for its simplicity and uniform architecture, and consists of 16 weight layers, including 13 convolutional layers and 3 fully connected layers.\n",
        "\n",
        "Let's use VGG16 with the dogs vs. cats example, starting by reusing the data module from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1ZUdQQhET5Ld"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import lightning as L\n",
        "from torchmetrics import Accuracy\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchinfo import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import opendatasets as od\n",
        "import os, shutil, pathlib\n",
        "\n",
        "torch.random.manual_seed(17);\n",
        "\n",
        "class DogsVsCatsDataModule(L.LightningDataModule):\n",
        "    def __init__(self, data_path='./datasets', transform = transforms.Compose([\n",
        "            transforms.Resize(size=(128, 128)),\n",
        "            transforms.ToTensor()\n",
        "        ]), train_transform=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.train_transform = transform if train_transform is None else train_transform\n",
        "\n",
        "    def make_subset(self, subset_name, start_index, end_index):\n",
        "        for category in (\"cat\", \"dog\"):\n",
        "            dir = self.dest_dir / subset_name / category\n",
        "            os.makedirs(dir)\n",
        "            fnames = [ f\"{category}.{i}.jpg\" for i in range(start_index, end_index) ]\n",
        "            for fname in fnames:\n",
        "                shutil.copyfile(src=self.src_dir / fname, dst=dir / fname)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        dataset_url = 'https://www.kaggle.com/c/dogs-vs-cats/data'\n",
        "        od.download(dataset_url, data_dir=self.data_path)\n",
        "\n",
        "        self.src_dir = pathlib.Path(self.data_path + \"/dogs-vs-cats/train\")\n",
        "        self.dest_dir = pathlib.Path(self.data_path + \"/dogs-vs-cats/processed\")\n",
        "\n",
        "        if not os.path.exists(self.dest_dir):\n",
        "            self.make_subset(\"train\", start_index=0, end_index=8000)\n",
        "            self.make_subset(\"validation\", start_index=8000, end_index=9500)\n",
        "            self.make_subset(\"test\", start_index=9500, end_index=11000)\n",
        "        else:\n",
        "            print(\"Skipping!\", self.dest_dir, \"already exists.\")\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.target_transform = transforms.Lambda(lambda y: torch.tensor([y]).float())\n",
        "\n",
        "        self.ds_train = datasets.ImageFolder(f\"{self.dest_dir}/train\", transform=self.train_transform)\n",
        "        self.ds_val = datasets.ImageFolder(f\"{self.dest_dir}/validation\", transform=self.transform)\n",
        "        self.ds_test = datasets.ImageFolder(f\"{self.dest_dir}/test\", transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.ds_train, batch_size=128, num_workers=4, shuffle=True,\n",
        "                          persistent_workers=True, drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.ds_val, batch_size=128, num_workers=4, persistent_workers=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.ds_test, batch_size=128, num_workers=4, persistent_workers=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DieDhtV9T5Lf"
      },
      "source": [
        "Next we download and instantiate a pre-trained vgg16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BqUTwv-_T5Lf",
        "outputId": "cc65617d-3117-4ef2-9f14-b81a928602e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:09<00:00, 60.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgps9KymT5Lf"
      },
      "source": [
        "Here is a summary of it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XU0KLc_-T5Lf",
        "outputId": "8c69d884-a932-48e5-d386-3257930abdcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "VGG                                      --\n",
              "├─Sequential: 1-1                        --\n",
              "│    └─Conv2d: 2-1                       1,792\n",
              "│    └─ReLU: 2-2                         --\n",
              "│    └─Conv2d: 2-3                       36,928\n",
              "│    └─ReLU: 2-4                         --\n",
              "│    └─MaxPool2d: 2-5                    --\n",
              "│    └─Conv2d: 2-6                       73,856\n",
              "│    └─ReLU: 2-7                         --\n",
              "│    └─Conv2d: 2-8                       147,584\n",
              "│    └─ReLU: 2-9                         --\n",
              "│    └─MaxPool2d: 2-10                   --\n",
              "│    └─Conv2d: 2-11                      295,168\n",
              "│    └─ReLU: 2-12                        --\n",
              "│    └─Conv2d: 2-13                      590,080\n",
              "│    └─ReLU: 2-14                        --\n",
              "│    └─Conv2d: 2-15                      590,080\n",
              "│    └─ReLU: 2-16                        --\n",
              "│    └─MaxPool2d: 2-17                   --\n",
              "│    └─Conv2d: 2-18                      1,180,160\n",
              "│    └─ReLU: 2-19                        --\n",
              "│    └─Conv2d: 2-20                      2,359,808\n",
              "│    └─ReLU: 2-21                        --\n",
              "│    └─Conv2d: 2-22                      2,359,808\n",
              "│    └─ReLU: 2-23                        --\n",
              "│    └─MaxPool2d: 2-24                   --\n",
              "│    └─Conv2d: 2-25                      2,359,808\n",
              "│    └─ReLU: 2-26                        --\n",
              "│    └─Conv2d: 2-27                      2,359,808\n",
              "│    └─ReLU: 2-28                        --\n",
              "│    └─Conv2d: 2-29                      2,359,808\n",
              "│    └─ReLU: 2-30                        --\n",
              "│    └─MaxPool2d: 2-31                   --\n",
              "├─AdaptiveAvgPool2d: 1-2                 --\n",
              "├─Sequential: 1-3                        --\n",
              "│    └─Linear: 2-32                      102,764,544\n",
              "│    └─ReLU: 2-33                        --\n",
              "│    └─Dropout: 2-34                     --\n",
              "│    └─Linear: 2-35                      16,781,312\n",
              "│    └─ReLU: 2-36                        --\n",
              "│    └─Dropout: 2-37                     --\n",
              "│    └─Linear: 2-38                      4,097,000\n",
              "=================================================================\n",
              "Total params: 138,357,544\n",
              "Trainable params: 138,357,544\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "summary(vgg16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF2rYFMCT5Lg"
      },
      "source": [
        "As you can see, there are two main big blocks separated by an `AdaptiveAvgPool2d` layer. You can think of the first sequential block as the base (or feature extractor) and of the second sequential block as the top (or classifier). Transfer learning involves changing/replacing the top block to fit the new task at hand. To do that, we freeze the network first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WDvO2ZuQT5Lg"
      },
      "outputs": [],
      "source": [
        "for param in vgg16.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNmaN1MgT5Lh"
      },
      "source": [
        "This makes sure that none of the pre-trained weights and biases will be affected when the model is trained again on the new data. Next, we change or replace the classifier block, which looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BOub8rJoT5Lh",
        "outputId": "5d24895a-3001-4630-cf8a-25de0826d0d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "vgg16.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgA10-LqT5Lh"
      },
      "source": [
        "To replace the last layer of the classifier with one that has two units instead of the original 1000, we do something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3s2CQ5o1T5Lh",
        "outputId": "59564131-18ce-4bb8-c018-f8a4aa4949c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=2, bias=True)\n",
        "vgg16.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPoSSDw2T5Li"
      },
      "source": [
        "And here is how to replace the while classifier block with a new untrained one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7-wzrqMUT5Li",
        "outputId": "622cd40e-18fd-4842-d2f8-b5885ffc8dd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vgg16.classifier = nn.Sequential(\n",
        "    nn.Linear(in_features=25088, out_features=256, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5, inplace=False),\n",
        "    nn.Linear(in_features=256, out_features=2, bias=True)\n",
        ")\n",
        "\n",
        "vgg16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUbv_ZaCT5Li"
      },
      "source": [
        "Now that we know how to do that, let's put it all inside a Lightning module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BVQkgyTmT5Li"
      },
      "outputs": [],
      "source": [
        "class VGG16BasedClassifier(L.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "\n",
        "        self.pretrained_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "        self.pretrained_model.eval()\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.pretrained_model.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=25088, out_features=256, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=256, out_features=2, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pretrained_model(x)\n",
        "\n",
        "    def _common_step(self, batch, batch_idx, accuracy, loss_lbl, accuracy_lbl):\n",
        "        X, y = batch\n",
        "        logits = self(X)\n",
        "        loss = nn.functional.cross_entropy(logits, y)\n",
        "        y_hat = torch.argmax(logits, dim=1)\n",
        "        self.log(loss_lbl, loss, prog_bar=True)\n",
        "        self.log(accuracy_lbl, accuracy(y_hat, y), prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._common_step(batch, batch_idx, self.train_accuracy, \"loss\", \"accuracy\")\n",
        "\n",
        "    def on_training_epoch_end(self):\n",
        "        self.log(\"accuracy\", self.train_accuracy.compute())\n",
        "        self.train_accuracy.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._common_step(batch, batch_idx, self.val_accuracy, \"val_loss\", \"val_accuracy\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\"val_accuracy\", self.val_accuracy.compute())\n",
        "        self.val_accuracy.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self._common_step(batch, batch_idx, self.test_accuracy, \"test_loss\", \"test_accuracy\")\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.log(\"test_accuracy\", self.test_accuracy.compute())\n",
        "        self.test_accuracy.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.RMSprop(self.parameters(), lr=1e-4)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ9_oHA1T5Lj"
      },
      "source": [
        "Using this class, here is a new model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HPUpXnpWT5Lj"
      },
      "outputs": [],
      "source": [
        "vgg16_based_model = VGG16BasedClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFf5jRe0T5Lj"
      },
      "source": [
        "Let's summarize it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I8m74OQKT5Lj",
        "outputId": "86f7896a-73e7-4ca5-a557-282b3a447d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "VGG16BasedClassifier                     --\n",
              "├─MulticlassAccuracy: 1-1                --\n",
              "├─MulticlassAccuracy: 1-2                --\n",
              "├─MulticlassAccuracy: 1-3                --\n",
              "├─VGG: 1-4                               --\n",
              "│    └─Sequential: 2-1                   --\n",
              "│    │    └─Conv2d: 3-1                  (1,792)\n",
              "│    │    └─ReLU: 3-2                    --\n",
              "│    │    └─Conv2d: 3-3                  (36,928)\n",
              "│    │    └─ReLU: 3-4                    --\n",
              "│    │    └─MaxPool2d: 3-5               --\n",
              "│    │    └─Conv2d: 3-6                  (73,856)\n",
              "│    │    └─ReLU: 3-7                    --\n",
              "│    │    └─Conv2d: 3-8                  (147,584)\n",
              "│    │    └─ReLU: 3-9                    --\n",
              "│    │    └─MaxPool2d: 3-10              --\n",
              "│    │    └─Conv2d: 3-11                 (295,168)\n",
              "│    │    └─ReLU: 3-12                   --\n",
              "│    │    └─Conv2d: 3-13                 (590,080)\n",
              "│    │    └─ReLU: 3-14                   --\n",
              "│    │    └─Conv2d: 3-15                 (590,080)\n",
              "│    │    └─ReLU: 3-16                   --\n",
              "│    │    └─MaxPool2d: 3-17              --\n",
              "│    │    └─Conv2d: 3-18                 (1,180,160)\n",
              "│    │    └─ReLU: 3-19                   --\n",
              "│    │    └─Conv2d: 3-20                 (2,359,808)\n",
              "│    │    └─ReLU: 3-21                   --\n",
              "│    │    └─Conv2d: 3-22                 (2,359,808)\n",
              "│    │    └─ReLU: 3-23                   --\n",
              "│    │    └─MaxPool2d: 3-24              --\n",
              "│    │    └─Conv2d: 3-25                 (2,359,808)\n",
              "│    │    └─ReLU: 3-26                   --\n",
              "│    │    └─Conv2d: 3-27                 (2,359,808)\n",
              "│    │    └─ReLU: 3-28                   --\n",
              "│    │    └─Conv2d: 3-29                 (2,359,808)\n",
              "│    │    └─ReLU: 3-30                   --\n",
              "│    │    └─MaxPool2d: 3-31              --\n",
              "│    └─AdaptiveAvgPool2d: 2-2            --\n",
              "│    └─Sequential: 2-3                   --\n",
              "│    │    └─Linear: 3-32                 6,422,784\n",
              "│    │    └─ReLU: 3-33                   --\n",
              "│    │    └─Dropout: 3-34                --\n",
              "│    │    └─Linear: 3-35                 514\n",
              "=================================================================\n",
              "Total params: 21,137,986\n",
              "Trainable params: 6,423,298\n",
              "Non-trainable params: 14,714,688\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "summary(vgg16_based_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prdWs1J9T5Lk"
      },
      "source": [
        "We can now train this model. When we do so, only the replaced classifier block will be affected. The rest is frozen. We'll use the early stopping callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715,
          "referenced_widgets": [
            "49add077aaf747719ef3d3691c76af07",
            "1b018333e7af41e4aa62fafb1a88b8c8",
            "77323239a76d4941ac8f6b0862fe1592",
            "6d46b64d7db2492c89cd354b02093bcb",
            "4fba67f9d581478a8fd781dd35128c46",
            "276da1d9ad7b448ab601c1d507b43bcb",
            "630f3fd7ebd84fc9bd54f1afb82f639b",
            "bba737badc1a4f268966b6d14100dd4b",
            "04d638cb43df4fe5bcd92efd96646648",
            "f7b6fc93071d4284a012c1b9633aa236",
            "1ae5ecadc09149a4a028650e3554ef39",
            "ee0ee103b7e346a9a69a0a01adfdd564",
            "2a70d12df8c848bb830059681861cf58",
            "d1c16d2712ac4af4b08b49c0ec50b104",
            "d6b7217eb43246078b8a37f00dfe57ee",
            "70fcde8902c14cf98f6973b9fa1aa2f4",
            "0bd6fd3e844c460792cfb3f9d5d19e5b",
            "efbd2842cfc445e1b7631b4659696298",
            "420a0a25fe4d406f9e7b06ccdc1e7524",
            "2c658f871cd44448afb60d27e9fe2e7c",
            "e27999f8e14d4585bb380165f421a478",
            "7d97dd7d2bc54d229165b5c1d28aed24"
          ]
        },
        "id": "4GrPm_-LmU3t",
        "outputId": "c79338ee-735c-4c3a-df2b-44b2f47a3c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
            "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: \n",
            "  | Name             | Type               | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | train_accuracy   | MulticlassAccuracy | 0      | train\n",
            "1 | val_accuracy     | MulticlassAccuracy | 0      | train\n",
            "2 | test_accuracy    | MulticlassAccuracy | 0      | train\n",
            "3 | pretrained_model | VGG                | 21.1 M | eval \n",
            "----------------------------------------------------------------\n",
            "6.4 M     Trainable params\n",
            "14.7 M    Non-trainable params\n",
            "21.1 M    Total params\n",
            "84.552    Total estimated model params size (MB)\n",
            "8         Modules in train mode\n",
            "34        Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name             | Type               | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | train_accuracy   | MulticlassAccuracy | 0      | train\n",
            "1 | val_accuracy     | MulticlassAccuracy | 0      | train\n",
            "2 | test_accuracy    | MulticlassAccuracy | 0      | train\n",
            "3 | pretrained_model | VGG                | 21.1 M | eval \n",
            "----------------------------------------------------------------\n",
            "6.4 M     Trainable params\n",
            "14.7 M    Non-trainable params\n",
            "21.1 M    Total params\n",
            "84.552    Total estimated model params size (MB)\n",
            "8         Modules in train mode\n",
            "34        Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49add077aaf747719ef3d3691c76af07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee0ee103b7e346a9a69a0a01adfdd564"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks import EarlyStopping\n",
        "\n",
        "dogs_vs_cats_dm = DogsVsCatsDataModule()\n",
        "\n",
        "trainer = L.Trainer(max_epochs=5, callbacks=[\n",
        "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "])\n",
        "trainer.fit(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538,
          "referenced_widgets": [
            "c4c7f297e8d04292bf68ba19ee2e779a",
            "f1a1175c4c3b42d4a84cd97d6932cc64",
            "cbf3cf3223454827940820925d5231f7",
            "ef4f269918914a30af721ce573445538",
            "8a405c15fb83497aa9651c60a877a83c",
            "94c238923ff54b66a89253266cb5e647",
            "a088d3cdcba14259800cc97065fb5791",
            "a217d89c1c2d4afe86c899245d3fb5cf",
            "557c90f106234ee282cce33c5938e76b",
            "5b2a96faa0bc40d7a7abed02601b9a80",
            "24ca5023f1424d7391d43550d3f9a22a",
            "",
            "95823f151ee24258a2845cab3936a795"
          ]
        },
        "outputId": "781789ba-b9b4-4590-eac9-91e622ef385c",
        "id": "LVUH0EjKT5Lk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
            "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name             | Type               | Params\n",
            "--------------------------------------------------------\n",
            "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
            "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
            "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
            "3 | pretrained_model | VGG                | 21.1 M\n",
            "--------------------------------------------------------\n",
            "6.4 M     Trainable params\n",
            "14.7 M    Non-trainable params\n",
            "21.1 M    Total params\n",
            "84.552    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |                                                                                   | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95823f151ee24258a2845cab3936a795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |                                                                                          | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks import EarlyStopping\n",
        "\n",
        "dogs_vs_cats_dm = DogsVsCatsDataModule()\n",
        "\n",
        "trainer = L.Trainer(max_epochs=5, callbacks=[\n",
        "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "])\n",
        "trainer.fit(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVdqpN0WT5Lk"
      },
      "source": [
        "Let's evaluate this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yki4hzZ5T5Ll",
        "outputId": "210b5239-8610-4e90-f0a7-6d3115b5d0dd",
        "colab": {
          "referenced_widgets": [
            "3efe7dc971e54955bd0e66e5fb32a7f0"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
            "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3efe7dc971e54955bd0e66e5fb32a7f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |                                                                                           | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9393333196640015     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14621175825595856    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9393333196640015    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14621175825595856   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 0.14621175825595856, 'test_accuracy': 0.9393333196640015}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqe3M_tzT5Ll"
      },
      "source": [
        "As you can see, this model significantly outperforms our previous from-scratch models by capitalizing on a pre-trained VGG16 model. The VGG16 model has been previously trained on the extensive ImageNet dataset, a widely used dataset for training and evaluating computer vision models, especially for image classification tasks. Given the richness of cat and dog images in the ImageNet dataset, this pretrained model proves highly effective in our specific task.\n",
        "\n",
        "\n",
        "\n",
        "### Fine-tuning a pretrained model\n",
        "As an optional, but widely used, step, we can improve our this pre-trained model by fine-tuning it. This is done by unfreezing all or part of the pre-trained base block and re-training it on the new data with a very slow learning rate. The whole process translates to the following steps:\n",
        "* Instantiate the base model\n",
        "* Freeze it\n",
        "* Add a new top to it\n",
        "* Train the part we added\n",
        "* Unfreeze some layers on the base model. Don't unfreeze any batch normalization layer.\n",
        "* Jointly train both the new unfrozen part of the base and the top part of the model using a very slow learning rate.\n",
        "\n",
        "We already did the top four steps. Let's do the last two. First we unfreeze the top two convolutional layers of the base block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lZTzy5tT5Ll"
      },
      "outputs": [],
      "source": [
        "for i in [26, 28]:\n",
        "    for param in vgg16_based_model.pretrained_model.features[i].parameters():\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpL2400RT5Ll"
      },
      "source": [
        "Having unfrozen these two layers, we train the model again with a slower learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSGSpy-4T5Lm",
        "outputId": "69b8ffe5-2a5d-4854-9538-dce7018b4bdf",
        "colab": {
          "referenced_widgets": [
            "",
            "ff3cf40bea9e418fa8bfd8a0cf2e7a85"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name             | Type               | Params\n",
            "--------------------------------------------------------\n",
            "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
            "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
            "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
            "3 | pretrained_model | VGG                | 21.1 M\n",
            "--------------------------------------------------------\n",
            "11.1 M    Trainable params\n",
            "10.0 M    Non-trainable params\n",
            "21.1 M    Total params\n",
            "84.552    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
            "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |                                                                                   | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff3cf40bea9e418fa8bfd8a0cf2e7a85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |                                                                                          | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
          ]
        }
      ],
      "source": [
        "vgg16_based_model.lr = 0.0001\n",
        "\n",
        "trainer = L.Trainer(max_epochs=2, callbacks=[\n",
        "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "])\n",
        "trainer.fit(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkmWlYVjT5Lm"
      },
      "source": [
        "Finally, let's evaluate this fine-tuned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMruGP5zT5Ln",
        "outputId": "da2256fb-3a05-457c-daa5-204112afa59d",
        "colab": {
          "referenced_widgets": [
            "8c8a7b011164403ba5376b638dda6c7d"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./datasets/dogs-vs-cats\" (use force=True to force download)\n",
            "Skipping! datasets/dogs-vs-cats/processed already exists.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c8a7b011164403ba5376b638dda6c7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |                                                                                           | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9380000233650208     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1642094999551773     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9380000233650208    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1642094999551773    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 0.1642094999551773, 'test_accuracy': 0.9380000233650208}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(vgg16_based_model, datamodule=dogs_vs_cats_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTDMd_hBT5Ln"
      },
      "source": [
        "## Cancer tissue detection\n",
        "As a second example, let's use another popular pre-trained network for detecting cancer tissues using the PCam dataset. ResNet50 is a variant of the ResNet (Residual Network) architecture, a deep neural network architecture that introduced residual blocks, which help address the challenges of training very deep neural networks. ResNet50 consists of 50 layers, making it a relatively deep neural network. Due to its success and efficiency, it serves as a benchmark model in the field of deep learning.\n",
        "\n",
        "Let's download and summarize a pre-trained ResNet50 model from `torchvision.models`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q1pDd77T5Lo",
        "outputId": "ce0b5331-c81e-4927-e8e0-c7c26f125502"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "ResNet                                   --\n",
              "├─Conv2d: 1-1                            9,408\n",
              "├─BatchNorm2d: 1-2                       128\n",
              "├─ReLU: 1-3                              --\n",
              "├─MaxPool2d: 1-4                         --\n",
              "├─Sequential: 1-5                        --\n",
              "│    └─Bottleneck: 2-1                   --\n",
              "│    │    └─Conv2d: 3-1                  4,096\n",
              "│    │    └─BatchNorm2d: 3-2             128\n",
              "│    │    └─Conv2d: 3-3                  36,864\n",
              "│    │    └─BatchNorm2d: 3-4             128\n",
              "│    │    └─Conv2d: 3-5                  16,384\n",
              "│    │    └─BatchNorm2d: 3-6             512\n",
              "│    │    └─ReLU: 3-7                    --\n",
              "│    │    └─Sequential: 3-8              16,896\n",
              "│    └─Bottleneck: 2-2                   --\n",
              "│    │    └─Conv2d: 3-9                  16,384\n",
              "│    │    └─BatchNorm2d: 3-10            128\n",
              "│    │    └─Conv2d: 3-11                 36,864\n",
              "│    │    └─BatchNorm2d: 3-12            128\n",
              "│    │    └─Conv2d: 3-13                 16,384\n",
              "│    │    └─BatchNorm2d: 3-14            512\n",
              "│    │    └─ReLU: 3-15                   --\n",
              "│    └─Bottleneck: 2-3                   --\n",
              "│    │    └─Conv2d: 3-16                 16,384\n",
              "│    │    └─BatchNorm2d: 3-17            128\n",
              "│    │    └─Conv2d: 3-18                 36,864\n",
              "│    │    └─BatchNorm2d: 3-19            128\n",
              "│    │    └─Conv2d: 3-20                 16,384\n",
              "│    │    └─BatchNorm2d: 3-21            512\n",
              "│    │    └─ReLU: 3-22                   --\n",
              "├─Sequential: 1-6                        --\n",
              "│    └─Bottleneck: 2-4                   --\n",
              "│    │    └─Conv2d: 3-23                 32,768\n",
              "│    │    └─BatchNorm2d: 3-24            256\n",
              "│    │    └─Conv2d: 3-25                 147,456\n",
              "│    │    └─BatchNorm2d: 3-26            256\n",
              "│    │    └─Conv2d: 3-27                 65,536\n",
              "│    │    └─BatchNorm2d: 3-28            1,024\n",
              "│    │    └─ReLU: 3-29                   --\n",
              "│    │    └─Sequential: 3-30             132,096\n",
              "│    └─Bottleneck: 2-5                   --\n",
              "│    │    └─Conv2d: 3-31                 65,536\n",
              "│    │    └─BatchNorm2d: 3-32            256\n",
              "│    │    └─Conv2d: 3-33                 147,456\n",
              "│    │    └─BatchNorm2d: 3-34            256\n",
              "│    │    └─Conv2d: 3-35                 65,536\n",
              "│    │    └─BatchNorm2d: 3-36            1,024\n",
              "│    │    └─ReLU: 3-37                   --\n",
              "│    └─Bottleneck: 2-6                   --\n",
              "│    │    └─Conv2d: 3-38                 65,536\n",
              "│    │    └─BatchNorm2d: 3-39            256\n",
              "│    │    └─Conv2d: 3-40                 147,456\n",
              "│    │    └─BatchNorm2d: 3-41            256\n",
              "│    │    └─Conv2d: 3-42                 65,536\n",
              "│    │    └─BatchNorm2d: 3-43            1,024\n",
              "│    │    └─ReLU: 3-44                   --\n",
              "│    └─Bottleneck: 2-7                   --\n",
              "│    │    └─Conv2d: 3-45                 65,536\n",
              "│    │    └─BatchNorm2d: 3-46            256\n",
              "│    │    └─Conv2d: 3-47                 147,456\n",
              "│    │    └─BatchNorm2d: 3-48            256\n",
              "│    │    └─Conv2d: 3-49                 65,536\n",
              "│    │    └─BatchNorm2d: 3-50            1,024\n",
              "│    │    └─ReLU: 3-51                   --\n",
              "├─Sequential: 1-7                        --\n",
              "│    └─Bottleneck: 2-8                   --\n",
              "│    │    └─Conv2d: 3-52                 131,072\n",
              "│    │    └─BatchNorm2d: 3-53            512\n",
              "│    │    └─Conv2d: 3-54                 589,824\n",
              "│    │    └─BatchNorm2d: 3-55            512\n",
              "│    │    └─Conv2d: 3-56                 262,144\n",
              "│    │    └─BatchNorm2d: 3-57            2,048\n",
              "│    │    └─ReLU: 3-58                   --\n",
              "│    │    └─Sequential: 3-59             526,336\n",
              "│    └─Bottleneck: 2-9                   --\n",
              "│    │    └─Conv2d: 3-60                 262,144\n",
              "│    │    └─BatchNorm2d: 3-61            512\n",
              "│    │    └─Conv2d: 3-62                 589,824\n",
              "│    │    └─BatchNorm2d: 3-63            512\n",
              "│    │    └─Conv2d: 3-64                 262,144\n",
              "│    │    └─BatchNorm2d: 3-65            2,048\n",
              "│    │    └─ReLU: 3-66                   --\n",
              "│    └─Bottleneck: 2-10                  --\n",
              "│    │    └─Conv2d: 3-67                 262,144\n",
              "│    │    └─BatchNorm2d: 3-68            512\n",
              "│    │    └─Conv2d: 3-69                 589,824\n",
              "│    │    └─BatchNorm2d: 3-70            512\n",
              "│    │    └─Conv2d: 3-71                 262,144\n",
              "│    │    └─BatchNorm2d: 3-72            2,048\n",
              "│    │    └─ReLU: 3-73                   --\n",
              "│    └─Bottleneck: 2-11                  --\n",
              "│    │    └─Conv2d: 3-74                 262,144\n",
              "│    │    └─BatchNorm2d: 3-75            512\n",
              "│    │    └─Conv2d: 3-76                 589,824\n",
              "│    │    └─BatchNorm2d: 3-77            512\n",
              "│    │    └─Conv2d: 3-78                 262,144\n",
              "│    │    └─BatchNorm2d: 3-79            2,048\n",
              "│    │    └─ReLU: 3-80                   --\n",
              "│    └─Bottleneck: 2-12                  --\n",
              "│    │    └─Conv2d: 3-81                 262,144\n",
              "│    │    └─BatchNorm2d: 3-82            512\n",
              "│    │    └─Conv2d: 3-83                 589,824\n",
              "│    │    └─BatchNorm2d: 3-84            512\n",
              "│    │    └─Conv2d: 3-85                 262,144\n",
              "│    │    └─BatchNorm2d: 3-86            2,048\n",
              "│    │    └─ReLU: 3-87                   --\n",
              "│    └─Bottleneck: 2-13                  --\n",
              "│    │    └─Conv2d: 3-88                 262,144\n",
              "│    │    └─BatchNorm2d: 3-89            512\n",
              "│    │    └─Conv2d: 3-90                 589,824\n",
              "│    │    └─BatchNorm2d: 3-91            512\n",
              "│    │    └─Conv2d: 3-92                 262,144\n",
              "│    │    └─BatchNorm2d: 3-93            2,048\n",
              "│    │    └─ReLU: 3-94                   --\n",
              "├─Sequential: 1-8                        --\n",
              "│    └─Bottleneck: 2-14                  --\n",
              "│    │    └─Conv2d: 3-95                 524,288\n",
              "│    │    └─BatchNorm2d: 3-96            1,024\n",
              "│    │    └─Conv2d: 3-97                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-98            1,024\n",
              "│    │    └─Conv2d: 3-99                 1,048,576\n",
              "│    │    └─BatchNorm2d: 3-100           4,096\n",
              "│    │    └─ReLU: 3-101                  --\n",
              "│    │    └─Sequential: 3-102            2,101,248\n",
              "│    └─Bottleneck: 2-15                  --\n",
              "│    │    └─Conv2d: 3-103                1,048,576\n",
              "│    │    └─BatchNorm2d: 3-104           1,024\n",
              "│    │    └─Conv2d: 3-105                2,359,296\n",
              "│    │    └─BatchNorm2d: 3-106           1,024\n",
              "│    │    └─Conv2d: 3-107                1,048,576\n",
              "│    │    └─BatchNorm2d: 3-108           4,096\n",
              "│    │    └─ReLU: 3-109                  --\n",
              "│    └─Bottleneck: 2-16                  --\n",
              "│    │    └─Conv2d: 3-110                1,048,576\n",
              "│    │    └─BatchNorm2d: 3-111           1,024\n",
              "│    │    └─Conv2d: 3-112                2,359,296\n",
              "│    │    └─BatchNorm2d: 3-113           1,024\n",
              "│    │    └─Conv2d: 3-114                1,048,576\n",
              "│    │    └─BatchNorm2d: 3-115           4,096\n",
              "│    │    └─ReLU: 3-116                  --\n",
              "├─AdaptiveAvgPool2d: 1-9                 --\n",
              "├─Linear: 1-10                           2,049,000\n",
              "=================================================================\n",
              "Total params: 25,557,032\n",
              "Trainable params: 25,557,032\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "summary(resnet50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMuslueT5Lo"
      },
      "source": [
        "As you can see, this is a deeper neural network. We can also print it to get the indexes and names of the layers and blocks that make it. This will be useful when we make changes to its top layer(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-yJ7qfcPT5Lo",
        "outputId": "2f7c5d64-e649-49f8-9c06-1463e1ccf677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(resnet50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpeCEDY0T5Lp"
      },
      "source": [
        "Looking at the last layer, its name is `fc` and it consists of 1000 units because of the 1000 classes of the ImageNet dataset it was trained on. This is the layer we need to replace to make this network work for our PCam dataset. Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKj3zHAWT5Lp"
      },
      "source": [
        "### Fetching and preparing the data\n",
        "\n",
        "We'll start by fetching and preparing the data, leveraging some of the code we had in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSur581YT5Lp",
        "outputId": "24464d97-ae09-4ed1-8ee9-5ef3f6ad1686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./datasets/histopathologic-cancer-detection\" (use force=True to force download)\n",
            "Skipping! datasets/histopathologic-cancer-detection/processed already exists.\n"
          ]
        }
      ],
      "source": [
        "def make_subset(subset, start_index, end_index, images, labels):\n",
        "    categories = {0: \"0_normal\", 1: \"1_abnormal\" }\n",
        "    for i in range(start_index, end_index):\n",
        "        category = categories[labels[i]]\n",
        "        dir = dest_dir / subset / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        fname = f\"{images[i]}.tif\"\n",
        "        shutil.copyfile(src=src_dir / fname, dst=dir / fname)\n",
        "\n",
        "# Downloading the data from kaggle if needed\n",
        "dataset_url = 'https://www.kaggle.com/c/histopathologic-cancer-detection'\n",
        "od.download(dataset_url, data_dir='./datasets')\n",
        "\n",
        "# Getting the actual labels\n",
        "data_path = './datasets/histopathologic-cancer-detection'\n",
        "labels = pd.read_csv(data_path + '/train_labels.csv')\n",
        "labels = labels.set_index('id')\n",
        "\n",
        "# Selecting random 20,000 images and splitting them into three sets\n",
        "src_dir = pathlib.Path(data_path + \"/train\")\n",
        "dest_dir = pathlib.Path(data_path + \"/processed\")\n",
        "\n",
        "train_image_files = os.listdir(src_dir)\n",
        "selected_images = [\n",
        "    train_image_files[f].split('.')[0]\n",
        "    for f in torch.randperm(len(train_image_files))[:20000]\n",
        "]\n",
        "\n",
        "selected_labels = labels.loc[selected_images]['label'].values\n",
        "\n",
        "if not os.path.exists(dest_dir):\n",
        "    make_subset(\"train\", 0, 16000, selected_images, selected_labels)\n",
        "    make_subset(\"validation\", 16000, 18000, selected_images, selected_labels)\n",
        "    make_subset(\"test\", 18000, 20000, selected_images, selected_labels)\n",
        "else:\n",
        "    print(\"Skipping!\", dest_dir, \"already exists.\")\n",
        "\n",
        "# Creating the datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "ds_train = datasets.ImageFolder(f\"{dest_dir}/train\", transform=train_transform)\n",
        "ds_val = datasets.ImageFolder(f\"{dest_dir}/validation\", transform=transform)\n",
        "ds_test = datasets.ImageFolder(f\"{dest_dir}/test\", transform=transform)\n",
        "\n",
        "# Creating the corresponding data loaders\n",
        "dl_train = DataLoader(ds_train, batch_size=256, num_workers=4,\n",
        "                      shuffle=True, persistent_workers=True,\n",
        "                      drop_last=True, pin_memory=True)\n",
        "dl_val = DataLoader(ds_val, batch_size=256, num_workers=4,\n",
        "                    persistent_workers=True, pin_memory=True)\n",
        "dl_test = DataLoader(ds_test, batch_size=256, num_workers=4,\n",
        "                     persistent_workers=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX8Ap8PhT5Lq"
      },
      "source": [
        "### Using a ResNet50 model\n",
        "\n",
        "Next, we construct a Lightning model based on ResNet50. Similar to our previous approach, once we download and instantiate the pretrained ResNet50, we freeze all its parameters. This prevents the pretrained weights from being updated during the training on the PCam dataset. Finally, we replace the last (top) linear layer, named `fc`, with a new one containing only 2 units instead of the original 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVRcr7LpT5Lq"
      },
      "outputs": [],
      "source": [
        "class ResNet50BasedClassifier(L.LightningModule):\n",
        "\n",
        "    def __init__(self, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=2)\n",
        "\n",
        "        self.pretrained_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        self.pretrained_model.eval()\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.pretrained_model.fc = nn.Linear(2048, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pretrained_model(x)\n",
        "\n",
        "    def _common_step(self, batch, batch_idx, accuracy, loss_lbl, accuracy_lbl):\n",
        "        X, y = batch\n",
        "        logits = self(X)\n",
        "        loss = nn.functional.cross_entropy(logits, y)\n",
        "        y_hat = torch.argmax(logits, dim=1)\n",
        "        self.log(loss_lbl, loss, prog_bar=True)\n",
        "        self.log(accuracy_lbl, accuracy(y_hat, y), prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._common_step(batch, batch_idx, self.train_accuracy, \"loss\", \"accuracy\")\n",
        "\n",
        "    def on_training_epoch_end(self):\n",
        "        self.log(\"accuracy\", self.train_accuracy.compute())\n",
        "        self.train_accuracy.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._common_step(batch, batch_idx, self.val_accuracy, \"val_loss\", \"val_accuracy\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\"val_accuracy\", self.val_accuracy.compute())\n",
        "        self.val_accuracy.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self._common_step(batch, batch_idx, self.test_accuracy, \"test_loss\", \"test_accuracy\")\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.log(\"test_accuracy\", self.test_accuracy.compute())\n",
        "        self.test_accuracy.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfJ7FMI0T5Lq"
      },
      "source": [
        "Now, we can train this model on the PCam dataset. It's important to note that we should not expect the same level of performance as in the previous example because the PCam data differs significantly from the ImageNet dataset on which the base ResNet50 model is trained. However with enough epochs, we still anticipate good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538,
          "referenced_widgets": [
            "c4c7f297e8d04292bf68ba19ee2e779a",
            "f1a1175c4c3b42d4a84cd97d6932cc64",
            "cbf3cf3223454827940820925d5231f7",
            "ef4f269918914a30af721ce573445538",
            "8a405c15fb83497aa9651c60a877a83c",
            "94c238923ff54b66a89253266cb5e647",
            "a088d3cdcba14259800cc97065fb5791",
            "a217d89c1c2d4afe86c899245d3fb5cf",
            "557c90f106234ee282cce33c5938e76b",
            "5b2a96faa0bc40d7a7abed02601b9a80",
            "24ca5023f1424d7391d43550d3f9a22a",
            "",
            "41231da45abb45dbb4fe2541733d4962"
          ]
        },
        "outputId": "781789ba-b9b4-4590-eac9-91e622ef385c",
        "id": "A_agVtztT5Lq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name             | Type               | Params\n",
            "--------------------------------------------------------\n",
            "0 | train_accuracy   | MulticlassAccuracy | 0     \n",
            "1 | val_accuracy     | MulticlassAccuracy | 0     \n",
            "2 | test_accuracy    | MulticlassAccuracy | 0     \n",
            "3 | pretrained_model | ResNet             | 23.5 M\n",
            "--------------------------------------------------------\n",
            "4.1 K     Trainable params\n",
            "23.5 M    Non-trainable params\n",
            "23.5 M    Total params\n",
            "94.049    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |                                                                                   | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41231da45abb45dbb4fe2541733d4962",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |                                                                                          | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |                                                                                        | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "resnet50_based_model = ResNet50BasedClassifier()\n",
        "\n",
        "trainer = L.Trainer(max_epochs=15, callbacks=[\n",
        "    EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
        "])\n",
        "\n",
        "trainer.fit(resnet50_based_model, train_dataloaders=dl_train, val_dataloaders=dl_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO0eHy12T5Lr"
      },
      "source": [
        "Finally, we evaluate this new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FyXPg3QT5Lr",
        "outputId": "92c55dfe-f934-48e1-d18a-ddfbdb84e2c5",
        "colab": {
          "referenced_widgets": [
            "034f319f24a14edbb3a95ce92aa906d1"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "034f319f24a14edbb3a95ce92aa906d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |                                                                                           | 0/? [00…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7854999899864197     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.47510087490081787    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7854999899864197    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.47510087490081787   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 0.47510087490081787, 'test_accuracy': 0.7854999899864197}]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(resnet50_based_model, dataloaders=dl_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEQEajzAT5Lr"
      },
      "source": [
        "These results are comparable, if not slightly better, than the ones obtained from models trained from scratch. This suggests that even with a substantially different dataset, such as PCam, transfer learning remains a powerful and effective technique."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Happy learning!"
      ],
      "metadata": {
        "id": "g-O9N6jnlr0l"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "0c4ab92bbc10d18b480eb40b47e751a2f458ae1b2b88576453c716fde62785ad"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49add077aaf747719ef3d3691c76af07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b018333e7af41e4aa62fafb1a88b8c8",
              "IPY_MODEL_77323239a76d4941ac8f6b0862fe1592",
              "IPY_MODEL_6d46b64d7db2492c89cd354b02093bcb"
            ],
            "layout": "IPY_MODEL_4fba67f9d581478a8fd781dd35128c46"
          }
        },
        "1b018333e7af41e4aa62fafb1a88b8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_276da1d9ad7b448ab601c1d507b43bcb",
            "placeholder": "​",
            "style": "IPY_MODEL_630f3fd7ebd84fc9bd54f1afb82f639b",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "77323239a76d4941ac8f6b0862fe1592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bba737badc1a4f268966b6d14100dd4b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04d638cb43df4fe5bcd92efd96646648",
            "value": 2
          }
        },
        "6d46b64d7db2492c89cd354b02093bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7b6fc93071d4284a012c1b9633aa236",
            "placeholder": "​",
            "style": "IPY_MODEL_1ae5ecadc09149a4a028650e3554ef39",
            "value": " 2/2 [01:16&lt;00:00,  0.03it/s]"
          }
        },
        "4fba67f9d581478a8fd781dd35128c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "276da1d9ad7b448ab601c1d507b43bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630f3fd7ebd84fc9bd54f1afb82f639b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bba737badc1a4f268966b6d14100dd4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04d638cb43df4fe5bcd92efd96646648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7b6fc93071d4284a012c1b9633aa236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ae5ecadc09149a4a028650e3554ef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee0ee103b7e346a9a69a0a01adfdd564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a70d12df8c848bb830059681861cf58",
              "IPY_MODEL_d1c16d2712ac4af4b08b49c0ec50b104",
              "IPY_MODEL_d6b7217eb43246078b8a37f00dfe57ee"
            ],
            "layout": "IPY_MODEL_70fcde8902c14cf98f6973b9fa1aa2f4"
          }
        },
        "2a70d12df8c848bb830059681861cf58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bd6fd3e844c460792cfb3f9d5d19e5b",
            "placeholder": "​",
            "style": "IPY_MODEL_efbd2842cfc445e1b7631b4659696298",
            "value": "Epoch 0:   1%"
          }
        },
        "d1c16d2712ac4af4b08b49c0ec50b104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_420a0a25fe4d406f9e7b06ccdc1e7524",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c658f871cd44448afb60d27e9fe2e7c",
            "value": 1
          }
        },
        "d6b7217eb43246078b8a37f00dfe57ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e27999f8e14d4585bb380165f421a478",
            "placeholder": "​",
            "style": "IPY_MODEL_7d97dd7d2bc54d229165b5c1d28aed24",
            "value": " 1/125 [00:38&lt;1:19:42,  0.03it/s, v_num=0, loss=0.750, accuracy=0.469]"
          }
        },
        "70fcde8902c14cf98f6973b9fa1aa2f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0bd6fd3e844c460792cfb3f9d5d19e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efbd2842cfc445e1b7631b4659696298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "420a0a25fe4d406f9e7b06ccdc1e7524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c658f871cd44448afb60d27e9fe2e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e27999f8e14d4585bb380165f421a478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d97dd7d2bc54d229165b5c1d28aed24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}